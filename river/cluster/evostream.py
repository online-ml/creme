import math
import random
from abc import ABCMeta

import numpy as np

from river import base, utils


class evoStream(base.Clusterer):
    r"""evoStream

    evoStream [^1] is a fairly new approach to utilize the idle time to incrementally
    build and refine micro clusters. This allows efficient time utilization and
    can support the offline component or replace it entirely, depending on
    the stream's speed. To do so, it employs an evolutionary algorithm, i.e
    a heuristic optimization, in order to find better macro-cluster solutions.

    Evolutionary algorithms, first introduced by Maulik U. and Bandyopadhyay S. [^2], are
    inspired by natural evolution where promising solutions are combined to create
    offsprings which can combine the best arrtributes of both parents.

    In River, since data arrives continuously, we will replace the concept of "idle" time
    with an evolution gap. This means that, after each gap, a temporary batch will be
    formed, the evolution function will be called, simulating the effect of "idle"
    time between batches of data points.

    The algorithm is divided into two main parts:

    **Online micro-cluster maintenance (learning)**

    For a new point `p`:

    * Create a new micro cluster from point `p`. Find all micro clusters whose
    centers fall within the fixed radius (clustering threshold) from the newly
    generated cluster

    * If one or more neighboring micro clusters are found, these micro clusters
    will be updated, trying to move them closer to the new instance (similar to the
    DBSTREAM algorithm [^3]). Else, that new micro cluster will
    be added to the current.

    * After each cleanup interval, weights of all micro clusters are updated once
    again. At this time, if the weight of any cluster is less than a certain threshold,
    it will be removed. Then, all clusters whose centers are less than `r` from
    each other are merged.

    * When the stream is not initiated and the number of cluster is at least equal
    to the initialization threshold, a cluster population of size `P` will be generated
    by choosing randomly micro-clusters from the set of currently available micro-clusters.

    **Offline generation of macro clusters (evolutionary step)**

    The offline generation of macro clusters is based upon the original GA clustering algorithm,
    as follows:

    * Using the roulette wheel algorithm, choose 2 arbitrary cluster solutions proportionally
    to their fitness from the cluster population.

    * Generate two offsprings using binary crossover.

    * Generate a random number `delta` within the range `[0,1]`. If `delta < P_m`, with `P_m` being
    the mutation probability, each gene `g_i` within these two offsprings will be mutated with
    the following rules:

        - If `g_i = 0`, `g_i` will be equal to either `-2 * delta` or `2 * delta`, with equal probability

        - If `g_i != 0`, `2 * delta * g_i` will be added or subtracted from `g_i`, with equal probability

    * The two newly generated solutions will be added to the set of cluster population. Two solutions with
    highest SSQ (or lowest fitness) will be discarded.

    * Choose the solution with the highest fitness and assign it as the clusters for the whole algorithm.

    Parameters
    ----------
    radius
        Radius threshold to determine neighboring clusters.
    decay_rate
        Parameter that controls the importance of historical data to current cluster.
        Note that `decay_rate` is usually sufficiently small and has to be different from `0`.
        We also note that `decay_rate` should be sufficiently small and greater than `0`.
    cleanup_interval
        The time interval between two consecutive time points when the cleanup process is
        conducted.
    evolution_interval
        The time interval between two consecutive time points when the evolution is conducted
        (i.e when the stream is "idle"). Besides, the algorithm will also choose the clustering
        solution with highest fitness to facilitate later predictions.
    initialization_threshold
        The minimum number of micro clusters required to initialize the choice of cluster
        solutions.
    mutation_probability
        Probability that the offsprings of two selected solution from the cluster population
        will be mutated. This probability has to be between `0` and `1`.
    population_size
        Size of the cluster population generated from the set of micro clusters.
    n_clusters
        Number of clusters generated by the algorithm.

    Attributes
    ----------
    clusters
        A set of final clusters of type `evoStreamMicroCluster`, i.e clusters with structure
        `(center, last_update, weight)`. These clusters are formed by choosing the cluster solution
        with the highest fitness.
    centers
        Final clusters' centers.

    References
    ----------
    [^1]: Carnein, M., Trautmann, H. (2018). evoStream – Evolutionary Stream Clustering Utilizing Idle Times.
          Big Data Research, 14, 101-111. DOI: 10.1016/j.bdr.2018.05.005
    [^2]: Maulik, U., Bandyopadhyay, S. (2000). Genetic algorithm-based clustering technique.
          Pattern Recognition, 33(9), 1455-1465. DOI: 10.1016/s0031-3203(99)00137-5
    [^3]: Hahsler, M., Bolanos, M. Clustering Data Streams Based on Shared Density between Micro-Clusters,
          IEEE Transactions on Knowledge and Data Engineering 28(6), 2016, 1449-1461.
          In Proceedings of the Sixth SIAM International Conference on Data Mining,
          April 20–22, 2006, Bethesda, MD, USA.

    Examples
    ----------
    >>> from river import cluster
    >>> from river import stream

    >>> X = [
    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1], [1, 1.125], [1, 1.25],
    ...     [1, 1.5], [1, 1.75], [1, 2], [4, 1.25], [4, 1.5], [4, 2.25],
    ...     [4, 2.5], [4, 3], [4, 3.25], [4, 3.5], [4, 3.75], [4, 4],
    ... ]

    >>> evostream = cluster.evoStream(radius=0.25,
    ...                               cleanup_interval=6,
    ...                               evolution_interval=7,
    ...                               n_clusters=2)

    >>> for x, _ in stream.iter_array(X):
    ...     evostream = evostream.learn_one(x)

    >>> evostream.predict_one({0: 0, 1: 0}) != evostream.predict_one({0: 4, 1: 3})
    True

    """

    def __init__(
        self,
        radius: float = 1.0,
        decay_rate: float = 0.01,
        cleanup_interval: int = 6,
        evolution_interval: int = 4,
        initialization_threshold: int = 3,
        mutation_probability: float = 0.5,
        population_size: int = 4,
        n_clusters: int = 2,
    ):
        super().__init__()
        self.time_stamp = 0

        self.radius = radius
        self.decay_rate = decay_rate
        self.cleanup_interval = cleanup_interval
        self.evolution_interval = evolution_interval
        self.initialization_threshold = initialization_threshold
        self.mutation_probability = mutation_probability
        self.population_size = population_size
        self.n_clusters = n_clusters
        self.initialized = False

        self.clusters = {}
        self.centers = {}

        self._evolution_batch = {}
        self._micro_clusters = {}
        self._cluster_population = {}
        self._fitness_cluster_population = {}

    @staticmethod
    def _binary_crossover(cluster_sol_1, cluster_sol_2):
        dim = len(cluster_sol_1[0].center)
        cutoff = random.randint(0, dim-1)
        for i in cluster_sol_1:
            for j in range(cutoff, dim):
                cluster_sol_1[i].center[j], cluster_sol_2[i].center[j] = (
                    cluster_sol_2[i].center[j],
                    cluster_sol_1[i].center[j],
                )
        return cluster_sol_1, cluster_sol_2

    @staticmethod
    def _distance(point_a, point_b):
        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))

    def _find_closest_cluster_index(self, point, clusters):
        min_distance = math.inf
        closest_cluster_index = -1
        for i, cluster_i in clusters.items():
            distance = self._distance(clusters[i].center, point)
            if distance < min_distance:
                min_distance = distance
                closest_cluster_index = i
        return closest_cluster_index

    def _gaussian_neighborhood(self, point_a, point_b):
        distance = self._distance(point_a, point_b)
        gaussian_neighborhood = math.exp(
            -((distance * distance) / (3 * (self.radius * self.radius))) / 2
        )
        return gaussian_neighborhood

    def _update(self, x):

        self.time_stamp += 1

        evolution_index = self.time_stamp % self.evolution_interval

        if evolution_index == 0:
            self._evolution_batch[self.evolution_interval - 1] = x
        elif evolution_index == 1:
            self._evolution_batch = {0: x}
        else:
            self._evolution_batch[evolution_index - 1] = x

        temp_mc = evoStreamMicroCluster(x=x, last_update=self.time_stamp, weight=1)

        merged_status = False

        for micro_cluster in self._micro_clusters.values():
            if self._distance(temp_mc.center, micro_cluster.center) < self.radius:
                magnitude = self._gaussian_neighborhood(
                    temp_mc.center, micro_cluster.center
                )
                for i in micro_cluster.center:
                    micro_cluster.center[i] += magnitude * (
                        temp_mc.center[i] - micro_cluster.center[i]
                    )
                micro_cluster.last_update = self.time_stamp
                micro_cluster.weight = (
                    micro_cluster.weight
                    * (
                        2
                        ** (
                            -self.decay_rate
                            * (self.time_stamp - micro_cluster.last_update)
                        )
                    )
                    + 1
                )
                merged_status = True

        if not merged_status:
            self._micro_clusters[len(self._micro_clusters)] = temp_mc

    def _fitness(self, points, clusters):
        ssq = 0
        for i, point_i in points.items():
            closest_cluster_index_i = self._find_closest_cluster_index(
                point_i, clusters
            )
            distance = self._distance(point_i, clusters[closest_cluster_index_i].center)
            ssq += distance * distance
        return 1 / ssq

    def _roulette_wheel_selection(self, fitness_clusters):
        sum_fitness = sum(fitness_clusters.values())
        pick_1, pick_2 = random.uniform(0, sum_fitness), random.uniform(0, sum_fitness)
        key_1, key_2 = -1, -1
        current = 0

        for key, fitness in fitness_clusters.items():
            current += fitness
            if current > pick_1 and key_1 == -1:
                key_1 = key
            if current > pick_2 and key_2 == -1:
                key_2 = key
            if key_1 != -1 and key_2 != -1:
                break

        return self._cluster_population[key_1], self._cluster_population[key_2]

    def _evolution(self):
        self._fitness_cluster_population = {
            i: self._fitness(self._evolution_batch, self._cluster_population[i])
            for i in self._cluster_population
        }

        p_1, p_2 = self._roulette_wheel_selection(self._fitness_cluster_population)
        o_1, o_2 = self._binary_crossover(p_1, p_2)
        delta = random.uniform(0, 1)

        def mutate(g):
            for i in g:
                if g[i] == 0:
                    g[i] = 2 * delta if random.uniform(0, 1) < 0.5 else -2 * delta
                else:
                    g[i] = (
                        2 * delta * g[i]
                        if random.uniform(0, 1) < 0.5
                        else -2 * delta * g[i]
                    )
                return g

        if delta < self.mutation_probability:
            for i in o_1:
                o_1[i].center = mutate(o_1[i].center)
                o_2[i].center = mutate(o_2[i].center)

        fitness_o_1 = self._fitness(self._evolution_batch, o_1)
        fitness_o_2 = self._fitness(self._evolution_batch, o_2)

        if fitness_o_1 > min(self._fitness_cluster_population.values()):
            argmin_1 = np.argmin(list(self._fitness_cluster_population.values()))
            self._cluster_population[argmin_1] = o_1
            self._fitness_cluster_population[argmin_1] = fitness_o_1

        if fitness_o_2 > min(self._fitness_cluster_population.values()):
            argmin_2 = np.argmin(list(self._fitness_cluster_population.values()))
            self._cluster_population[argmin_2] = o_2
            self._fitness_cluster_population[argmin_2] = fitness_o_2

    def _merge_micro_clusters(self, micro_clusters):
        merged_status = {i: False for i in micro_clusters}
        merged_mcs = {}
        n_micro_cluster = len(micro_clusters)
        count = 0
        for i in micro_clusters:
            if merged_status[i]:
                continue
            else:
                merged_mcs[count] = micro_clusters[i]
                for j in range(i + 1, n_micro_cluster):
                    if (not merged_status[j]) and self._distance(
                        merged_mcs[count].center, micro_clusters[j].center
                    ) <= self.radius:
                        merged_mcs[count].merge(micro_clusters[j])
                        merged_status[j] = True
                count += 1
        return merged_mcs

    def _cleanup(self):
        # Algorithm 2 of Michael Hahsler and Matthew Bolanos: Cleanup process to remove
        # inactive clusters and shared density entries from memory

        for i in list(self._micro_clusters):
            self._micro_clusters[i].weight = self._micro_clusters[i].weight * (
                2
                ** (
                    -self.decay_rate
                    * (self.time_stamp - self._micro_clusters[i].last_update)
                )
            )
            if self._micro_clusters[i].weight < 2 ** (
                -self.decay_rate * self.cleanup_interval
            ):
                self._micro_clusters.pop(i)

        self._micro_clusters = {
            i: v for i, v in enumerate(self._micro_clusters.values())
        }

        self._micro_clusters = self._merge_micro_clusters(self._micro_clusters)

    @staticmethod
    def _generate_population(micro_clusters, population_size, n_clusters):
        cluster_population = {}
        for i in range(population_size):
            cluster_population_i_keys = random.sample(micro_clusters.keys(), n_clusters)
            cluster_population[i] = {
                i: micro_clusters[cluster_population_i_keys[i]]
                for i in range(n_clusters)
            }
        return cluster_population

    def learn_one(self, x, sample_weight=None):

        self._update(x)

        if self.time_stamp % self.cleanup_interval == 0:
            self._cleanup()

        # initialize macro clusters
        if (
            len(self._micro_clusters) == self.initialization_threshold
            and not self.initialized
        ):
            self._cluster_population = self._generate_population(
                self._micro_clusters, self.initialization_threshold, self.n_clusters
            )
            self.initialized = True

        if self.time_stamp % self.evolution_interval == 0:
            self._evolution()

            clusters_solution_index = np.argmax(
                list(self._fitness_cluster_population.values())
            )

            self.clusters = self._cluster_population[clusters_solution_index]

            self.centers = {i: self.clusters[i].center for i in self.clusters}

        return self

    def predict_one(self, x, sample_weight=None):

        label = self._find_closest_cluster_index(x, self.clusters)

        return label


class evoStreamMicroCluster(metaclass=ABCMeta):
    """ evoStream Micro-cluster class """

    def __init__(self, x=None, last_update=None, weight=None):

        self.center = x
        self.last_update = last_update
        self.weight = weight

    def merge(self, cluster):
        self.center = {
            i: (self.center[i] * self.weight + cluster.center[i] * cluster.weight)
            / (self.weight + cluster.weight)
            for i in self.center
        }
        self.weight += cluster.weight
        self.last_update = max(self.last_update, cluster.last_update)
