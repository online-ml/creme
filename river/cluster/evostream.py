import math
import random
from abc import ABCMeta

import numpy as np

from river import base, utils


class evoStream(base.Clusterer):
    r"""DBSTREAM

    DBSTREAM [^1] is a clustering algorithm for evolving data streams.
    It is the first micro-cluster-based online clustering component that
    explicitely captures the density between micro-clusters via a shared
    density graph. The density information in the graph is then exploited
    for reclustering based on actual density between adjacent micro clusters.
    The algorithm is divided into two parts:

    **Online micro-cluster maintenance (learning)**

    For a new point `p`:

    * Find all micro clusters for which `p` falls within the fixed radius
    (clustering threshold). If no neighbor is found, a new micro cluster
    with a weight of 1 is created for `p`.

    * If no neighbor is found, a new micro cluster with a weight of 1 is
    created for `p`. If one or more neighbors of `p` are found, we update
    the micro clusters by applying the appropriate fading, increasing
    their weight and then we try to move them closer to `p` using the
    Gaussian neighborhood function.

    * Next, the shared density graph is updated. To prevent collapsing
    micro clusters, we will restrict the movement for micro clusters in case
    they come closer than $r$ (clustering threshold) to each other. Finishing
    this process, the time stamp is also increased by 1.

    * Finally, the cleanup will be processed. It is executed every `t_gap`
    time steps, removing weak micro clusters and weak entries in the
    shared density graph to recover memory and improve the clustering algorithm's
    processing speed.

    **Offline generation of macro clusters (clustering)**

    The offline generation of macro clusters is generated through two following steps:

    * The connectivity graph `C` is constructed using shared density entries
    between strong micro clusters. The edges in this connectivity graph with
    a connectivity value greater than the intersection threshold ($\alpha$)
    are used to find connected components representing the final cluster.

    * After the connectivity graph is generated, a variant of DBSCAN algorithm
    proposed by Ester et al. is applied to form all macro clusters
    from $\alpha$-connected micro clusters.

    Parameters
    ----------
    radius
        DBStream represents each micro cluster by a leader (a data point defining the
        micro cluster's center) and the density in an area of a user-specified radius
        $r$ (`clustering_threshold`) around the center.
    decay_rate
        Parameter that controls the importance of historical data to current cluster.
        Note that `fading_factor` has to be different from `0`.
    cleanup_interval
        The time interval between two consecutive time points when the cleanup process is
         conducted.
    evolution_interval
        The minimum weight for a cluster to be not "noisy".
    initialization_threshold
        The intersection factor related to the area of the overlap of the micro clusters
        relative to the area cover by micro clusters. This parameter is used to determine
        whether a micro cluster or a shared density is weak.
    mutation_probability

    population_size

    n_clusters

    Attributes
    ----------
    n_clusters
        Number of clusters generated by the algorithm.
    clusters
        A set of final clusters of type `DBStreamMicroCluster`. However, these are either
        micro clusters, or macro clusters that are generated by merging all $\alpha$-connected
        micro clusters. This set is generated through the offline phase of the algorithm.
    centers
        Final clusters' centers.
    micro_clusters
        Micro clusters generated by the algorithm. Instead of updating directly the new instance points
        into a nearest micro cluster, through each iteration, the weight and center will be modified
        so that the clusters are closer to the new points, using the Gaussian neighborhood function.

    References
    ----------
    [^1]: Michael Hahsler and Matthew Bolanos (2016, pp 1449-1461). Clsutering Data Streams Based on Shared Density
          between Micro-Clusters, IEEE Transactions on Knowledge and Data Engineering (volume .
          In Proceedings of the Sixth SIAM International Conference on Data Mining,
          April 20â€“22, 2006, Bethesda, MD, USA.
    [^2]: Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
          with Noise. In KDD-96 Proceedings, AAAI.

    Examples
    ----------
    >>> from river import cluster
    >>> from river import stream

    >>> X = [
    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1.125], [1, 1.5], [1, 1.75],
    ...     [4, 1.25], [4, 1.5], [4, 2.25], [4, 2.5],
    ...     [4, 3], [4, 3.25], [4, 3.5], [4, 3.75], [4, 4]
    ... ]

    >>> evostream = cluster.evoStream(radius=0.25, evolution_interval=5, cleanup_interval=6, n_clusters=2)

    >>> for x, _ in stream.iter_array(X):
    ...     evostream = evostream.learn_one(x)

    >>> evostream.predict_one({0: 0, 1: 0}) != evostream.predict_one({0: 4, 1: 3})
    True

    """

    def __init__(
        self,
        radius: float = 1,
        decay_rate: float = 0.01,
        cleanup_interval: int = 6,
        evolution_interval: int = 4,
        initialization_threshold: int = 3,
        mutation_probability: float = 0.5,
        population_size: int = 4,
        n_clusters: int = 2,
    ):
        super().__init__()
        self.time_stamp = 0

        self.radius = radius
        self.decay_rate = decay_rate
        self.cleanup_interval = cleanup_interval
        self.evolution_interval = evolution_interval
        self.initialization_threshold = initialization_threshold
        self.mutation_probability = mutation_probability
        self.population_size = population_size
        self.n_clusters = n_clusters
        self.initialized = False

        self.clusters = {}
        self.centers = {}
        self.micro_clusters = {}
        self.cluster_population = {}
        self.evolution_batch = {}
        self.fitness_cluster_population = {}

    @staticmethod
    def _binary_crossover(cluster_sol_1, cluster_sol_2):
        dim = len(cluster_sol_1[0].center)
        cutoff = math.floor(dim / 2)
        for i in cluster_sol_1:
            for j in range(cutoff, dim):
                cluster_sol_1[i].center[j], cluster_sol_2[i].center[j] = (
                    cluster_sol_2[i].center[j],
                    cluster_sol_1[i].center[j],
                )
        return cluster_sol_1, cluster_sol_2

    @staticmethod
    def _distance(point_a, point_b):
        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))

    def _gaussian_neighborhood(self, point_a, point_b):
        distance = self._distance(point_a, point_b)
        gaussian_neighborhood = math.exp(
            -((distance * distance) / (3 * (self.radius * self.radius))) / 2
        )
        return gaussian_neighborhood

    def _update(self, x):

        self.time_stamp += 1

        evolution_index = self.time_stamp % self.evolution_interval

        if evolution_index == 0:
            self.evolution_batch[self.evolution_interval - 1] = x
        elif evolution_index == 1:
            self.evolution_batch = {0: x}
        else:
            self.evolution_batch[evolution_index - 1] = x

        temp_mc = evoStreamMicroCluster(x=x, last_update=self.time_stamp, weight=1)

        merged_status = False

        for micro_cluster in self.micro_clusters.values():
            if self._distance(temp_mc.center, micro_cluster.center) < self.radius:
                magnitude = self._gaussian_neighborhood(
                    temp_mc.center, micro_cluster.center
                )
                for i in micro_cluster.center.keys():
                    micro_cluster.center[i] += magnitude * (
                        temp_mc.center[i] - micro_cluster.center[i]
                    )
                micro_cluster.last_update = self.time_stamp
                micro_cluster.weight = (
                    micro_cluster.weight
                    * (
                        2
                        ** (
                            -self.decay_rate
                            * (self.time_stamp - micro_cluster.last_update)
                        )
                    )
                    + 1
                )
                merged_status = True

        if not merged_status:
            self.micro_clusters[len(self.micro_clusters)] = temp_mc

    def _find_closest_cluster_index(self, point, clusters):
        min_distance = math.inf
        closest_cluster_index = -1
        for i, cluster_i in clusters.items():
            distance = self._distance(clusters[i].center, point)
            if distance < min_distance:
                min_distance = distance
                closest_cluster_index = i
        return closest_cluster_index

    def _fitness(self, points, clusters):
        ssq = 0
        for i, point_i in points.items():
            closest_cluster_index_i = self._find_closest_cluster_index(
                point_i, clusters
            )
            distance = self._distance(point_i, clusters[closest_cluster_index_i].center)
            ssq += distance * distance
        return 1 / ssq

    def _roulette_wheel_selection(self, fitness_clusters):
        sum_fitness = sum(fitness_clusters.values())
        pick_1, pick_2 = random.uniform(0, sum_fitness), random.uniform(0, sum_fitness)
        key_1, key_2 = -1, -1
        current = 0

        for key, fitness in fitness_clusters.items():
            current += fitness
            if current > pick_1 and key_1 == -1:
                key_1 = key
            if current > pick_2 and key_2 == -1:
                key_2 = key
            if key_1 != -1 and key_2 != -1:
                break

        return self.cluster_population[key_1], self.cluster_population[key_2]

    def _evolution(self):
        self.fitness_cluster_population = {
            i: self._fitness(self.evolution_batch, self.cluster_population[i])
            for i in self.cluster_population.keys()
        }

        p_1, p_2 = self._roulette_wheel_selection(self.fitness_cluster_population)
        o_1, o_2 = self._binary_crossover(p_1, p_2)
        delta = random.uniform(0, 1)

        def mutate(g):
            for i in g.keys():
                if g[i] == 0:
                    g[i] = 2 * delta if random.uniform(0, 1) < 0.5 else -2 * delta
                else:
                    g[i] = (
                        2 * delta * g[i]
                        if random.uniform(0, 1) < 0.5
                        else -2 * delta * g[i]
                    )
                return g

        if delta < self.mutation_probability:
            for i in o_1:
                o_1[i].center = mutate(o_1[i].center)
                o_2[i].center = mutate(o_2[i].center)

        fitness_o_1 = self._fitness(self.evolution_batch, o_1)
        fitness_o_2 = self._fitness(self.evolution_batch, o_2)

        if fitness_o_1 > min(self.fitness_cluster_population.values()):
            argmin_1 = np.argmin(list(self.fitness_cluster_population.values()))
            self.cluster_population[argmin_1] = o_1
            self.fitness_cluster_population[argmin_1] = fitness_o_1

        if fitness_o_2 > min(self.fitness_cluster_population.values()):
            argmin_2 = np.argmin(list(self.fitness_cluster_population.values()))
            self.cluster_population[argmin_2] = o_2
            self.fitness_cluster_population[argmin_2] = fitness_o_2

    def _merge_micro_clusters(self, micro_clusters):
        merged_status = {i: False for i in micro_clusters.keys()}
        merged_mcs = {}
        n_micro_cluster = len(micro_clusters)
        count = 0
        for i in micro_clusters:
            if merged_status[i]:
                continue
            else:
                merged_mcs[count] = micro_clusters[i]
                for j in range(i + 1, n_micro_cluster):
                    if (not merged_status[j]) and self._distance(
                        merged_mcs[count].center, micro_clusters[j].center
                    ) <= self.radius:
                        merged_mcs[count].merge(micro_clusters[j])
                        merged_status[j] = True
                count += 1
        return merged_mcs

    def _cleanup(self):
        # Algorithm 2 of Michael Hahsler and Matthew Bolanos: Cleanup process to remove
        # inactive clusters and shared density entries from memory

        for i in list(self.micro_clusters.keys()):
            self.micro_clusters[i].weight = self.micro_clusters[i].weight * (
                2
                ** (
                    -self.decay_rate
                    * (self.time_stamp - self.micro_clusters[i].last_update)
                )
            )
            if self.micro_clusters[i].weight < 2 ** (
                -self.decay_rate * self.cleanup_interval
            ):
                self.micro_clusters.pop(i)

        self.micro_clusters = {i: v for i, v in enumerate(self.micro_clusters.values())}

        self.micro_clusters = self._merge_micro_clusters(self.micro_clusters)

    @staticmethod
    def _generate_population(micro_clusters, population_size, n_clusters):
        C = {}
        for i in range(population_size):
            C_i_keys = random.sample(micro_clusters.keys(), n_clusters)
            C_i = {i: micro_clusters[C_i_keys[i]] for i in range(n_clusters)}
            C[i] = C_i
        return C

    def learn_one(self, x, sample_weight=None):

        self._update(x)

        if self.time_stamp % self.cleanup_interval == 0:
            self._cleanup()

        # initialize macro clusters
        if (
            len(self.micro_clusters) == self.initialization_threshold
            and not self.initialized
        ):
            self.cluster_population = self._generate_population(
                self.micro_clusters, self.initialization_threshold, self.n_clusters
            )
            self.initialized = True

        if self.time_stamp % self.evolution_interval == 0:
            self._evolution()

            clusters_solution_index = np.argmax(
                list(self.fitness_cluster_population.values())
            )

            self.clusters = self.cluster_population[clusters_solution_index]

            self.centers = {i: self.clusters[i].center for i in self.clusters}

        return self

    def predict_one(self, x, sample_weight=None):
        min_distance = math.inf
        closest_cluster_index = -1
        for i, center_i in self.centers.items():
            distance = self._distance(center_i, x)
            if distance < min_distance:
                min_distance = distance
                closest_cluster_index = i

        return closest_cluster_index


class evoStreamMicroCluster(metaclass=ABCMeta):
    """ DBStream Micro-cluster class """

    def __init__(self, x=None, last_update=None, weight=None):

        self.center = x
        self.last_update = last_update
        self.weight = weight

    def merge(self, cluster):
        self.center = {
            i: (self.center[i] * self.weight + cluster.center[i] * cluster.weight)
            / (self.weight + cluster.weight)
            for i in self.center.keys()
        }
        self.weight += cluster.weight
        self.last_update = max(self.last_update, cluster.last_update)
